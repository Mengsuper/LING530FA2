{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_emotion = []\n",
    "train_tweets = []\n",
    "with open('dataset/train.csv') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"', skipinitialspace=True)\n",
    "    line_count = 0\n",
    "    for row in spamreader:\n",
    "        line_count += 1\n",
    "        if line_count == 1: continue # skip header\n",
    "        if not row: continue\n",
    "        emotion = row[0]\n",
    "        tweet = row[1]\n",
    "        tweet = tweet.replace('@USERNAME', '')\n",
    "        tweet = tweet.replace('[#TRIGGERWORD#]', '')\n",
    "        tweet = result = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "        train_tweets.append(tweet)\n",
    "        train_emotion.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = train_tweets\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in list(word_counts.items()) if e[1] > 2]\n",
    "vocabularySize = len(vocabulary)\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24358"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'disgust', 'fear', 'joy', 'sad', 'surprise']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create emotion array\n",
    "emotions = sorted(list(set(train_emotion)))\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentence, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a reference sentence, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = word_tokenize(reference_sentence.lower())\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu([reference_tokenized], predicted_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build a Emotion Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLSTM(\n",
       "  (lstm): LSTM(300, 300, bidirectional=True)\n",
       "  (out): Linear(in_features=600, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = False\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True) # <-change here\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size) # <- change here\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1) \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(2, 1, self.hidden_size)) # <- change here\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "'''\n",
    "# decoder for one hot embedding\n",
    "decoder=DecoderLSTM(input_size=len(vocabulary), \n",
    "                    hidden_size=300, \n",
    "                    output_size=len(emotions))\n",
    "'''\n",
    "# decoder for word2vec embedding\n",
    "decoder=DecoderLSTM(input_size=wordEncodingSize, \n",
    "                    hidden_size=300, \n",
    "                    output_size=len(emotions))\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the Emotion Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build some helper function\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 0s (- 1789m 16s) (0 0%) 0.116045\n",
      "3m 13s (- 490m 32s) (1000 0%) 0.101334\n"
     ]
    }
   ],
   "source": [
    "def train(target_variable, \n",
    "          emotion,\n",
    "          decoder, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=w2v_embeddings,\n",
    "          teacher_force=True): \n",
    "    \"\"\"\n",
    "    Given a single training sample, go through a single step of training.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[target_variable[0].data[0]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = (decoder.initHidden(), decoder.initHidden())\n",
    "\n",
    "    for di in range(0,target_variable.size(0)):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "\n",
    "        if teacher_force:\n",
    "            ni = target_variable[di].data[0]\n",
    "        else:          \n",
    "            ni = topi[0][0]\n",
    "        \n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        if di == target_variable.size(0) - 2: \n",
    "            loss += criterion(decoder_output, emotion)\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 10.0)\n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_variable.size(0)\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "num_epochs = 1\n",
    "numberized_emotion = [emotions.index(emotion) for emotion in train_emotion]\n",
    "target_emotion = Variable(torch.LongTensor(numberized_emotion))\n",
    "start = time.time()\n",
    "total_loss = 0\n",
    "avg_loss = []\n",
    "for _ in range(num_epochs):\n",
    "    for i,sentence in enumerate(train_tweets):\n",
    "        \n",
    "        numberized = preprocess_numberize(sentence)\n",
    "        if len(numberized) == 2:\n",
    "            continue\n",
    "        target_variable = Variable(torch.LongTensor(numberized[1:]))\n",
    "\n",
    "        loss = train(target_variable, target_emotion[i], decoder, decoder_optimizer, criterion)\n",
    "        total_loss += loss\n",
    "        avg_loss.append(total_loss/(i+1))\n",
    "        if i % 1000 == 0:\n",
    "            print('%s (%d %d%%) %.6f' % \n",
    "                  (timeSince(start, (i+1)/len(train_tweets)), i, (i+1)/len(train_tweets)*100, total_loss/(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_tweets)\n",
    "showPlot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after training, save model \n",
    "torch.save(decoder.state_dict(), 'decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load previously training model:\n",
    "torch.load(decoder.load_state_dict(), ('decoder.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate the Emotion decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_tweets = []\n",
    "with open('dataset/dev.csv') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"', skipinitialspace=True)\n",
    "    line_count = 0\n",
    "    for row in spamreader:\n",
    "        line_count += 1\n",
    "        if line_count == 1: continue # skip header\n",
    "        if not row: continue\n",
    "        tweet = row[1]\n",
    "        tweet = tweet.replace('@USERNAME', '')\n",
    "        tweet = tweet.replace('[#TRIGGERWORD#]', '')\n",
    "        tweet = result = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "        dev_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_emotions = []\n",
    "with open('dataset/trial-v3.csv') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"', skipinitialspace=True)\n",
    "    line_count = 0\n",
    "    for row in spamreader:\n",
    "        line_count += 1\n",
    "        if line_count == 1: continue # skip header\n",
    "        if not row: continue\n",
    "        dev_emotions.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth, model prediction\n",
      "0  correct predictions in  1\n",
      "acurray:  0.0\n",
      "36  correct predictions in  101\n",
      "acurray:  0.3564356435643564\n",
      "75  correct predictions in  201\n",
      "acurray:  0.373134328358209\n",
      "106  correct predictions in  301\n",
      "acurray:  0.3521594684385382\n",
      "138  correct predictions in  401\n",
      "acurray:  0.34413965087281795\n",
      "172  correct predictions in  501\n",
      "acurray:  0.34331337325349304\n",
      "213  correct predictions in  601\n",
      "acurray:  0.3544093178036606\n",
      "258  correct predictions in  701\n",
      "acurray:  0.3680456490727532\n",
      "298  correct predictions in  801\n",
      "acurray:  0.37203495630461925\n",
      "333  correct predictions in  901\n",
      "acurray:  0.36958934517203107\n",
      "374  correct predictions in  1001\n",
      "acurray:  0.37362637362637363\n",
      "413  correct predictions in  1101\n",
      "acurray:  0.3751135331516803\n",
      "447  correct predictions in  1201\n",
      "acurray:  0.37218984179850123\n",
      "484  correct predictions in  1301\n",
      "acurray:  0.372021521906226\n",
      "523  correct predictions in  1401\n",
      "acurray:  0.37330478229835834\n",
      "564  correct predictions in  1501\n",
      "acurray:  0.3757495003331113\n",
      "601  correct predictions in  1601\n",
      "acurray:  0.37539038101186756\n",
      "638  correct predictions in  1701\n",
      "acurray:  0.3750734861845973\n",
      "673  correct predictions in  1801\n",
      "acurray:  0.3736812881732371\n",
      "721  correct predictions in  1901\n",
      "acurray:  0.37927406628090476\n",
      "754  correct predictions in  2001\n",
      "acurray:  0.37681159420289856\n",
      "794  correct predictions in  2101\n",
      "acurray:  0.37791527843883865\n",
      "828  correct predictions in  2201\n",
      "acurray:  0.37619263970922306\n",
      "868  correct predictions in  2301\n",
      "acurray:  0.37722729248152975\n",
      "913  correct predictions in  2401\n",
      "acurray:  0.3802582257392753\n",
      "952  correct predictions in  2501\n",
      "acurray:  0.38064774090363857\n",
      "1005  correct predictions in  2601\n",
      "acurray:  0.3863898500576701\n",
      "1039  correct predictions in  2701\n",
      "acurray:  0.3846723435764532\n",
      "1079  correct predictions in  2801\n",
      "acurray:  0.385219564441271\n",
      "1110  correct predictions in  2901\n",
      "acurray:  0.3826266804550155\n",
      "1148  correct predictions in  3001\n",
      "acurray:  0.38253915361546154\n",
      "1186  correct predictions in  3101\n",
      "acurray:  0.38245727184779105\n",
      "1225  correct predictions in  3201\n",
      "acurray:  0.3826929084661043\n",
      "1270  correct predictions in  3301\n",
      "acurray:  0.3847318994244168\n",
      "1311  correct predictions in  3401\n",
      "acurray:  0.3854748603351955\n",
      "1359  correct predictions in  3501\n",
      "acurray:  0.38817480719794345\n",
      "1398  correct predictions in  3601\n",
      "acurray:  0.38822549291863373\n",
      "1439  correct predictions in  3701\n",
      "acurray:  0.3888138340988922\n",
      "1483  correct predictions in  3801\n",
      "acurray:  0.390160484083136\n",
      "1527  correct predictions in  3901\n",
      "acurray:  0.3914380927967188\n",
      "1566  correct predictions in  4001\n",
      "acurray:  0.39140214946263435\n",
      "1586  correct predictions in  4101\n",
      "acurray:  0.3867349426969032\n",
      "1607  correct predictions in  4201\n",
      "acurray:  0.38252796953106405\n",
      "1624  correct predictions in  4301\n",
      "acurray:  0.3775866077656359\n",
      "1635  correct predictions in  4401\n",
      "acurray:  0.3715064758009543\n",
      "1654  correct predictions in  4501\n",
      "acurray:  0.36747389469006886\n",
      "1674  correct predictions in  4601\n",
      "acurray:  0.363833949141491\n",
      "1698  correct predictions in  4701\n",
      "acurray:  0.36119974473516275\n",
      "1718  correct predictions in  4801\n",
      "acurray:  0.3578421162257863\n",
      "1737  correct predictions in  4901\n",
      "acurray:  0.35441746582330136\n",
      "1757  correct predictions in  5001\n",
      "acurray:  0.35132973405318935\n",
      "1776  correct predictions in  5101\n",
      "acurray:  0.34816702607331895\n",
      "1786  correct predictions in  5201\n",
      "acurray:  0.34339550086521825\n",
      "1804  correct predictions in  5301\n",
      "acurray:  0.3403131484625542\n",
      "1817  correct predictions in  5401\n",
      "acurray:  0.3364191816330309\n",
      "1829  correct predictions in  5501\n",
      "acurray:  0.33248500272677695\n",
      "1842  correct predictions in  5601\n",
      "acurray:  0.32886984467059455\n",
      "1855  correct predictions in  5701\n",
      "acurray:  0.3253815120154359\n",
      "1878  correct predictions in  5801\n",
      "acurray:  0.32373728667471124\n",
      "1892  correct predictions in  5901\n",
      "acurray:  0.3206236231147263\n",
      "1907  correct predictions in  6001\n",
      "acurray:  0.31778036993834363\n",
      "1920  correct predictions in  6101\n",
      "acurray:  0.31470250778560893\n",
      "1929  correct predictions in  6201\n",
      "acurray:  0.31107885824866954\n",
      "1942  correct predictions in  6301\n",
      "acurray:  0.3082050468179654\n",
      "1961  correct predictions in  6401\n",
      "acurray:  0.3063583815028902\n",
      "1982  correct predictions in  6501\n",
      "acurray:  0.30487617289647745\n",
      "1994  correct predictions in  6601\n",
      "acurray:  0.3020754431146796\n",
      "2013  correct predictions in  6701\n",
      "acurray:  0.3004029249365766\n",
      "2034  correct predictions in  6801\n",
      "acurray:  0.2990736656374063\n",
      "2047  correct predictions in  6901\n",
      "acurray:  0.2966236777278655\n",
      "2062  correct predictions in  7001\n",
      "acurray:  0.2945293529495786\n",
      "2076  correct predictions in  7101\n",
      "acurray:  0.29235318969159274\n",
      "2098  correct predictions in  7201\n",
      "acurray:  0.29134842383002363\n",
      "2113  correct predictions in  7301\n",
      "acurray:  0.28941240925900563\n",
      "2131  correct predictions in  7401\n",
      "acurray:  0.2879340629644643\n",
      "2145  correct predictions in  7501\n",
      "acurray:  0.28596187175043325\n",
      "2160  correct predictions in  7601\n",
      "acurray:  0.2841731351138008\n",
      "2179  correct predictions in  7701\n",
      "acurray:  0.2829502661991949\n",
      "2196  correct predictions in  7801\n",
      "acurray:  0.2815023714908345\n",
      "2207  correct predictions in  7901\n",
      "acurray:  0.27933173016073914\n",
      "2220  correct predictions in  8001\n",
      "acurray:  0.27746531683539555\n",
      "2243  correct predictions in  8101\n",
      "acurray:  0.27687939760523395\n",
      "2261  correct predictions in  8201\n",
      "acurray:  0.27569808559931713\n",
      "2269  correct predictions in  8301\n",
      "acurray:  0.2733405613781472\n",
      "2284  correct predictions in  8401\n",
      "acurray:  0.27187239614331626\n",
      "2294  correct predictions in  8501\n",
      "acurray:  0.26985060581108106\n",
      "2306  correct predictions in  8601\n",
      "acurray:  0.2681083594930822\n",
      "2325  correct predictions in  8701\n",
      "acurray:  0.26721066544075395\n",
      "2342  correct predictions in  8801\n",
      "acurray:  0.26610612430405634\n",
      "2357  correct predictions in  8901\n",
      "acurray:  0.26480170767329514\n",
      "2369  correct predictions in  9001\n",
      "acurray:  0.263192978557938\n",
      "2390  correct predictions in  9101\n",
      "acurray:  0.26260850455993845\n",
      "2409  correct predictions in  9201\n",
      "acurray:  0.2618193674600587\n",
      "2423  correct predictions in  9301\n",
      "acurray:  0.2605096226212235\n",
      "2440  correct predictions in  9401\n",
      "acurray:  0.2595468567173705\n",
      "2455  correct predictions in  9501\n",
      "acurray:  0.25839385327860226\n"
     ]
    }
   ],
   "source": [
    "def evaluate(decoder, \n",
    "             target_variable, \n",
    "             embeddings=w2v_embeddings, \n",
    "             teacher_force=True):\n",
    "    \n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[target_variable[0].data[0]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = (decoder.initHidden(),decoder.initHidden())\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    for di in range(0,target_variable.size(0)):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        \n",
    "        if teacher_force:\n",
    "            ni = target_variable[di].data[0]\n",
    "        else:          \n",
    "            ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        if di == target_variable.size(0) - 2: # last output \n",
    "            if dev_emotions[i] == emotions[topi[0][0]]:\n",
    "                return True\n",
    "            #print (dev_emotions[i], emotions[topi[0][0]])\n",
    "            \n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "    return False\n",
    "\n",
    "# evaluate the model\n",
    "print (\"ground truth, model prediction\")\n",
    "correct_prediction_counts = 0\n",
    "for i,tweet in enumerate(dev_tweets): \n",
    "    numberized = preprocess_numberize(tweet)\n",
    "    if len(numberized) == 2: continue\n",
    "    target_variable = Variable(torch.LongTensor(numberized[1:]))\n",
    "    \n",
    "    if evaluate(decoder, target_variable):\n",
    "        correct_prediction_counts += 1\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print (correct_prediction_counts, \" correct predictions in \", i+1)\n",
    "        print (\"acurray: \", correct_prediction_counts/(i+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
